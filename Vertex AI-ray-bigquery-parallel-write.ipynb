{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e6279-9796-4ec9-9bd2-d5c12638ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install xgboost_ray\n",
    "!pip install ray[data,train]==2.4.0\n",
    "!pip install google-cloud-aiplatform[ray]==1.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291412c-fb1d-46e6-a5dc-a6ee123ba2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This code segment utilizes the BigQuery Public Dataset \"bitcoin_blockchain.transactions,\" accessible at https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ml_datasets&page=dataset&ws=!1m5!1m4!4m3!1sbigquery-public-data!2sbitcoin_blockchain!3stransactions. It transfers this table to a project under your ownership, enabling the reading and writing of substantial datasets within a Ray on Vertex cluster.\n",
    "\n",
    "# The table being copied has the following details:\n",
    "\n",
    "# Number of rows: 340,311,544\n",
    "# Total logical bytes: 587.14 GB\n",
    "# Active logical bytes: 587.14 GB\n",
    "# For testing purposes, this table can be substituted with any other public BigQuery table.\n",
    "\n",
    "#\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "\n",
    "# define source BigQuery table\n",
    "SOURCE_PROJECT_ID = 'bigquery-public-data'\n",
    "SOURCE_DATASET_ID = 'bitcoin_blockchain'\n",
    "SOURCE_TABLE_ID = 'transactions'\n",
    "\n",
    "# define destination BigQuery table\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_PROJECT_ID = '<>'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "# create destination dataset\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "dataset_uri = f'{client.project}.{DESTINATION_DATASET_ID}'\n",
    "dataset = bigquery.Dataset(dataset_uri)\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "job_config = bigquery.CopyJobConfig()\n",
    "job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "# copy table to from source to destination, and overwrite it if it already exists\n",
    "job = client.copy_table(\n",
    "    f'{SOURCE_PROJECT_ID}.{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    f'{DESTINATION_PROJECT_ID}.{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "job.result()\n",
    "assert job.state == 'DONE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60adeaa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6ecc2-6b06-432c-b0e3-5e7319d31150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from google.cloud import aiplatform\n",
    "import vertex_ray\n",
    "from vertex_ray import get_ray_cluster, create_ray_cluster, Resources\n",
    "from google.cloud.aiplatform.utils import resource_manager_utils\n",
    "\n",
    "PROJECT_ID=\"<>\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PROJECT_NUMBER=resource_manager_utils.get_project_number(PROJECT_ID)\n",
    "STAGING_BUCKET = f\"gs://{PROJECT_ID}\"\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "CLUSTER_NAME=\"ray-cluster-bq-large-write-test-1\"\n",
    "\n",
    "NETWORK=\"default\"\n",
    "NETWORK_NAME=f\"projects/{PROJECT_NUMBER}/global/networks/{NETWORK}\"\n",
    "\n",
    "try:\n",
    "    cluster_resource_name = create_ray_cluster(\n",
    "        cluster_name=CLUSTER_NAME,\n",
    "        network=NETWORK_NAME,\n",
    "        ray_version=\"2_4\",\n",
    "        python_version=\"3_10\",\n",
    "        head_node_type=Resources(\n",
    "            machine_type=\"n1-standard-16\",\n",
    "            node_count=1,\n",
    "            accelerator_type=None,\n",
    "            accelerator_count=0,\n",
    "            boot_disk_size_gb=300\n",
    "        ),\n",
    "        worker_node_types=[Resources(\n",
    "            machine_type=\"n1-standard-16\",\n",
    "            node_count=10,\n",
    "            accelerator_type=None,\n",
    "            accelerator_count=0,\n",
    "            boot_disk_size_gb=300\n",
    "        )],\n",
    "    )\n",
    "    address = f\"vertex_ray://{cluster_resource_name}\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cluster = get_ray_cluster(f\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\")\n",
    "address = f\"vertex_ray://{cluster.cluster_resource_name}\"\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(\"ray\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "while ray.util.client.num_connected_contexts() > 0:\n",
    "    ray.shutdown()\n",
    "\n",
    "context = ray.init(\n",
    "    address = address,\n",
    "    runtime_env = {\n",
    "        \"pip\": [\"google-cloud-aiplatform[ray]==1.35.0\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "from datetime import datetime # datetime is used for creating the dataset name\n",
    "import time  # time is used for time benchmarking\n",
    "from vertex_ray import BigQueryDatasource\n",
    "\n",
    "PROJECT_ID=\"<>\"\n",
    "\n",
    "##### BEGIN EDITABLE FIELD #####\n",
    "SOURCE_DATASET_ID = 'rov_xgboost_dataset'\n",
    "SOURCE_TABLE_ID = 'bitcoin_blockchain_transactions'\n",
    "\n",
    "DESTINATION_LOCATION = 'US'\n",
    "DESTINATION_DATASET_ID = 'rov_xgboost_dataset'\n",
    "DESTINATION_TABLE_ID = 'bitcoin_blockchain_transactions_write'\n",
    "\n",
    "read_parallelism = 100\n",
    "write_parallelism = 10\n",
    "query_row_limit = 1000000\n",
    "#### END EDITABLE FIELD #####\n",
    "\n",
    "start_time = time.time()\n",
    "ds = ray.data.read_datasource(\n",
    "    BigQueryDatasource(),\n",
    "    parallelism=read_parallelism,\n",
    "    # dataset=f'{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}',\n",
    "    query=f'SELECT * FROM `{SOURCE_DATASET_ID}.{SOURCE_TABLE_ID}` LIMIT {query_row_limit}'\n",
    ")\n",
    "# Ray datasource reads are lazy, so the end time cannot be captured until ds.fully_executed() is called\n",
    "ds.fully_executed()\n",
    "print(\"block_num_rows:\", ds._block_num_rows())\n",
    "num_blocks = ds.num_blocks()\n",
    "print(\"num_blocks:\", num_blocks)\n",
    "end_time = time.time()\n",
    "print(\"[RoV debug] Read time:\", round(end_time - start_time, 2), \", Num rows:\", ds.count(), \", Num blocks:\", num_blocks)\n",
    "\n",
    "# delete destination table first\n",
    "client = bigquery.Client(project=DESTINATION_PROJECT_ID, location=DESTINATION_LOCATION)\n",
    "client.delete_table(f'{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}', not_found_ok=True)\n",
    "\n",
    "\n",
    "# repartition the dataset to a new number of blocks for writing to BigQuery\n",
    "ds = ds.repartition(write_parallelism).materialize()\n",
    "print(\"block_num_rows:\", ds._block_num_rows())\n",
    "num_blocks = ds.num_blocks()\n",
    "print(\"num_blocks:\", num_blocks)\n",
    "\n",
    "start_time = time.time()\n",
    "ds.write_datasource(\n",
    "    BigQueryDatasource(),\n",
    "    dataset=f'{DESTINATION_DATASET_ID}.{DESTINATION_TABLE_ID}',\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"[RoV debug] Write time:\", round(end_time - start_time, 2), \", Num rows:\", ds.count(), \", Num blocks:\", num_blocks)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4cd1c-94a2-4a8c-b115-7f170452e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Ray Cluster\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import vertex_ray\n",
    "from vertex_ray import delete_ray_cluster\n",
    "from google.cloud.aiplatform.utils import resource_manager_utils\n",
    "\n",
    "PROJECT_ID=\"<>\"\n",
    "REGION=\"us-central1\"\n",
    "\n",
    "PROJECT_NUMBER=resource_manager_utils.get_project_number(PROJECT_ID)\n",
    "STAGING_BUCKET = f\"gs://{PROJECT_ID}\"\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "CLUSTER_NAME=\"ray-cluster-bq-large-write-test-1\"\n",
    "\n",
    "cluster_resource_name = f\"projects/{PROJECT_NUMBER}/locations/{REGION}/persistentResources/{CLUSTER_NAME}\"\n",
    "\n",
    "try:\n",
    "    response = delete_ray_cluster(cluster_resource_name)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
